Title: What Game Theory Reveals About Life, The Universe, and Everything
Channel: Veritasium
Published: 2023-12-23T17:00:08Z
Duration: PT27M19S
Description: This is a video about the most famous problem in Game Theory, the Prisoner’s Dilemma. Head to https://brilliant.org/veritasium  to start your free 30-day trial, and the first 200 people get 20% off an annual premium subscription.

Special thanks to our Patreon supporters! Join the community to help us keep our videos free, forever: 
https://ve42.co/PatreonDEB

If you’re looking for a molecular modeling kit, try Snatoms – a kit I invented where the atoms snap together magnetically – https://ve42.co/SnatomsV   


▀▀▀
A massive thank you to Prof. Robert Axelrod and Prof. Steven Strogatz for their expertise and time. 

To read more about Prof. Axelrod’s Passion for Cooperation visit: https://ve42.co/Axelrod2023 

A massive thanks to the wonderful Nicky Case. Nicky’s “The Evolution of Trust” game was a huge inspiration for this video. We highly recommend you play this excellent game yourself, over at: https://ncase.me/trust/ 

A huge thank you to those who helped us understand and fact check different parts of this topic - Dr. Christian Hilbe, Dr. Vincent Knight, Dr. Jelena Grujic, Prof. Andreas Diekmann, and Dr. Alexander Stewart.

▀▀▀
References:
Excellent game on the evolution of trust by Nicky Case - https://ve42.co/Case2023  
Summary of Axelrod’s work by This Place - https://www.youtube.com/watch?v=BOvAbjfJ0x0 
How to outsmart the Prisoner’s Dilemma by TED-Ed - https://www.youtube.com/watch?v=emyi4z-O0ls&pp=ygUScHJpc29uZXIncyBkaWxlbW1h 
Tit for Tat by radiolab - https://ve42.co/T4T   
The Golden Rule by radiolab - https://ve42.co/GoldenRule   
Axelrod, R. (1984). The Evolution of Cooperation.
Dawkins, R. (2016). The selfish gene. Oxford university press.
Poundstone, W. (1992). Prisoner's Dilemma. William Poundstone.
Nowak, M. A., & Highfield, R. (2011). Supercooperators. Edinburgh: Canongate.
Binmore, K. (2007). Game theory: a very short introduction. OUP Oxford.
Northrup, L. & Rock, D. (1966). The Detection of Joe I. - https://ve42.co/JOE1 
Prisoner’s dilemma, Wikipedia - https://ve42.co/WikiPD
Prisoner’s Dilemma, Stanford -  https://ve42.co/StanfordPD 
Flood, M. M. (1952). Some experimental games. - https://ve42.co/Flood1952 
Historical nuclear weapons stockpiles, Wikipedia - https://ve42.co/WikiNWS 
Goodwin, I. (1998). The Price of Victory in Cold War - https://ve42.co/Goodwin1998 
Cold war: How it happened. - https://ve42.co/CW2014 
Axelrod, R. (1980). Effective choice in the prisoner's dilemma. Journal of conflict resolution, 24(1), 3-25. - https://ve42.co/Axelrod1980a 
Axelrod, R. (1980). More effective choice in the prisoner's dilemma. Journal of conflict resolution, 24(3), 379-403. - https://ve42.co/Axelrod1980b 
Axelrod, R., & Hamilton, W. D. (1981). The evolution of cooperation. science, 211(4489), 1390-1396. https://ve42.co/Axelrod1981 
Stanislav Petrov, Wikipedia - https://ve42.co/WikiSP 
Wu, J., & Axelrod, R. (1995). How to cope with noise in the iterated prisoner's dilemma. Journal of Conflict resolution, 39(1), 183-189. - https://ve42.co/Wu1995 
INF Treaty - https://ve42.co/INF 
START Treaties - https://ve42.co/START 
START I, Wikipedia - https://ve42.co/WikiSTART 


Images & Video:
RAND Historical images via rand.org - https://ve42.co/RAND  
Golden Balls - https://www.youtube.com/watch?v=S0qjK3TWZE8
Zotti, G., et al. (2021). The Simulated Sky: Stellarium for Cultural Astronomy Research - https://ve42.co/Stellarium  
Newspapers from 1980s via Newspapers.com  – https://ve42.co/Newspapers  
Decommisioned nuke image via The Moscow Times - https://ve42.co/MT2012 
Soviet inspection image via Bulletin of the Atomic Scientists - https://ve42.co/Krzyzaniak2019  
Decommissioning nuclear weapon via ShareAmerica - https://ve42.co/Kaufman2014 


▀▀▀
Special thanks to our Patreon supporters:
Adam Foreman, Amadeo Bee, Anton Ragin, Balkrishna Heroor, Bernard McGee, Bill Linder, Burt Humburg, Dave Kircher, Diffbot, Evgeny Skvortsov, Gnare, Jesse Brandsoy, John H. Austin, Jr., john kiehl, Josh Hibschman, Juan Benet, KeyWestr, Lee Redden, Marinus Kuivenhoven, Mario Bottion, Max Maladino, Meekay, meg noah, Michael Krugman, Paul Peijzel, Richard Sundvall, Sam Lutfi, Stephen Wilcox, Tj Steyn, TTST, Ubiquity Ventures


▀▀▀
Directed by Casper Mebius
Written by Casper Mebius, Derek Muller, Petr Lebedev, and Ashley Hamer
Additional research and fact checking by Gregor Čavlović and Will Wood
Edited by Peter Nelson
Animated by Fabio Albertelli, Ivy Tello and Alondra Vitae
Illustrations by Jakub Misiek
Filmed by Derek Muller
Produced by Casper Mebius, Derek Muller, Gregor Čavlović and Han Evans

Additional video/photos supplied by Getty Images
Music from Epidemic Sound
Thumbnail by Peter Sheppard

Transcript:

[00:00:00] - This is a video about
the most famous problem
[00:00:02] in game theory.
[00:00:03] Problems of this sort pop up
everywhere, from nations locked
[00:00:07] in conflict to roommates doing the dishes.
[00:00:10] Even game shows have been
based around this concept.
[00:00:13] Figuring out the best strategy
can mean the difference
[00:00:16] between life and death, war and peace,
[00:00:19] flourishing and the
destruction of the planet.
[00:00:22] And in the mechanics of this game,
[00:00:24] we may find the very source of one
[00:00:26] of the most unexpected phenomena
in nature: cooperation.
[00:00:30] (upbeat music)
[00:00:34] On the 3rd of September, 1949,
[00:00:36] an American weather monitoring plane
[00:00:38] collected air samples over Japan.
[00:00:41] In those samples, they found
traces of radioactive material.
[00:00:46] The Navy quickly collected
[00:00:47] and tested rainwater
samples from their ships
[00:00:49] and bases all over the world.
[00:00:52] They also detected small amounts
[00:00:53] of Cerium-141 and Yttrium-91.
[00:00:58] But these isotopes have half
lives of one or two months,
[00:01:02] so they must have been produced recently
[00:01:04] and the only place they
could have come from
[00:01:06] was a nuclear explosion.
[00:01:09] But the US hadn't performed
any tests that year,
[00:01:12] so the only possible conclusion
[00:01:15] was that the Soviet Union had figured out
[00:01:17] how to make a nuclear bomb.
[00:01:20] This was the news the
Americans had been dreading.
[00:01:23] Their military supremacy achieved
[00:01:25] through the Manhattan
Project was quickly fading.
[00:01:29] - This makes the problem of Western Europe
[00:01:31] and the United States far more
serious than it was before
[00:01:35] and perhaps makes the
imminence of war greater.
[00:01:37] - Some thought their
best course of action was
[00:01:39] to launch an unprovoked nuclear
strike against the Soviets
[00:01:43] while they were still ahead.
[00:01:45] In the words of Navy Secretary Matthews
[00:01:47] to become "aggressors for peace".
[00:01:51] John von Neuman, the
founder of game theory,
[00:01:53] said, "If you say why
not bomb them tomorrow,
[00:01:57] I say, why not bomb them today?
[00:01:59] If you say today at five o'clock,
[00:02:01] I say why not at one o'clock?"
[00:02:06] Something needed to be done
about nuclear weapons and fast.
[00:02:10] But what?
[00:02:11] In 1950, the RAND Corporation,
[00:02:14] a US-based think tank was
studying this question.
[00:02:17] And as part of this research,
they turned to game theory.
[00:02:21] That same year, two mathematicians
at RAND had invented
[00:02:24] a new game, one which
unbeknownst to them at the time,
[00:02:27] closely resembled the US-Soviet conflict.
[00:02:31] This game is now known as
the prisoner's dilemma.
[00:02:34] So let's play a game.
[00:02:36] A banker with a chest full
of gold coins invites you
[00:02:39] and another player to
play against each other.
[00:02:42] You each get two choices.
[00:02:43] You can cooperate or you can defect.
[00:02:46] If you both cooperate,
you each get three coins.
[00:02:50] If one of you cooperates,
[00:02:51] but the other defects,
[00:02:53] then the one who defected gets five coins
[00:02:55] and the other gets nothing.
[00:02:57] And if you both defect,
then you each get a coin.
[00:03:01] The goal of the game is simple:
[00:03:02] to get as many coins as you can.
[00:03:06] So what would you do?
[00:03:09] Suppose your opponent cooperates,
[00:03:11] then you could also
cooperate and get three coins
[00:03:15] or you could defect and
get five coins, instead.
[00:03:18] So you are better off defecting,
[00:03:21] but what if your opponent
defects, instead?
[00:03:24] Well, you could cooperate and get no coins
[00:03:27] or you could defect and
at least get one coin.
[00:03:31] So no matter what your opponent does,
[00:03:34] your best option is always to defect.
[00:03:37] Now, if your opponent is also rational,
[00:03:40] they will reach the same conclusion
[00:03:42] and therefore also defect.
[00:03:44] As a result, when you both act rationally,
[00:03:46] you both end up in the
suboptimal situation
[00:03:49] getting one coin each
[00:03:51] when you could have gotten three, instead.
[00:03:54] In the case of the US and Soviet Union,
[00:03:56] this led both countries to
develop huge nuclear arsenals
[00:04:00] of tens of thousands of
nuclear weapons each,
[00:04:03] more than enough to destroy
each other many times over.
[00:04:07] But since both countries had
nukes, neither could use them.
[00:04:11] And both countries spent
[00:04:12] around $10 trillion
developing these weapons.
[00:04:16] Both would've been better
off if they had cooperated
[00:04:19] and agreed not to develop
this technology further.
[00:04:22] But since they both acted
in their own best interest,
[00:04:25] they ended up in a situation
where everyone was worse off.
[00:04:30] The prisoner's dilemma is one
[00:04:32] of the most famous games in game theory.
[00:04:34] Thousands and thousands of
papers have been published
[00:04:37] on versions of this game.
[00:04:39] In part, that's because
it pops up everywhere.
[00:04:42] (bright music)
[00:04:44] Impalas living in
between African woodlands
[00:04:46] and Savannahs are prone to
catching ticks, which can lead
[00:04:50] to infectious diseases,
paralysis, even death.
[00:04:53] So it's important for
impalas to remove ticks
[00:04:56] and they do this by grooming,
[00:04:58] but they can't reach all
the spots on their bodies
[00:05:01] and therefore they need
another impala to groom them.
[00:05:05] Now, grooming someone
else comes at a cost.
[00:05:08] It costs saliva, electrolytes,
time and attention,
[00:05:11] all vital resources
under the hot African sun
[00:05:14] where a predator could
strike at any moment.
[00:05:17] So for the other impala,
it would be best not
[00:05:20] to pay this cost, but then again,
[00:05:22] it too will need help grooming.
[00:05:25] So all impalas face a choice:
[00:05:27] should they groom each other or not?
[00:05:29] In other words, should
they cooperate or defect?
[00:05:34] Well, if they only interact once,
[00:05:36] then the rational solution
is always to defect.
[00:05:39] That other impala is never
gonna help you, so why bother?
[00:05:43] But the thing about a lot of problems is
[00:05:45] that they're not a single
prisoner's dilemma.
[00:05:48] Impalas see each other day after day
[00:05:50] and the same situation keeps
happening over and over again.
[00:05:54] So that changes the problem
[00:05:56] because instead of playing the
prisoner's dilemma just once,
[00:05:59] you're now playing it many, many times.
[00:06:02] And if I defect now, then
my opponent will know
[00:06:05] that I'd defected and they can use
[00:06:07] that against me in the future.
[00:06:10] So what is the best strategy
in this repeated game?
[00:06:18] That is what Robert Axelrod,
[00:06:20] a political scientist wanted to find out.
[00:06:22] So in 1980 he decided to
hold a computer tournament.
[00:06:26] - He invited some of the
world's leading game theorists
[00:06:28] for many different subjects
to submit computer programs
[00:06:32] that would play each other.
[00:06:34] - Axelrod called these
programs strategies.
[00:06:38] Each strategy would face off
against every other strategy
[00:06:41] and against a copy of itself
[00:06:43] and each matchup would go for 200 rounds.
[00:06:47] That's important and
we'll come back to it.
[00:06:49] Now, Axelrod used points instead of coins,
[00:06:52] but the payoffs were the same.
[00:06:53] The goal of the tournament
was to win as many points
[00:06:56] as possible and in the end,
[00:06:58] the whole tournament was
repeated five times over
[00:07:01] to ensure the success was
robust and not just a fluke.
[00:07:06] Axelrod gave an example
of a simple strategy.
[00:07:09] It would start each game by
cooperating and only defect
[00:07:12] after its opponent had
defected twice in a row.
[00:07:16] In total Axelrod received 14 strategies
[00:07:19] and he added a 15th called random,
[00:07:21] which just randomly cooperates or defects
[00:07:23] 50% of the time.
[00:07:27] All strategies were loaded
onto a single computer
[00:07:29] where they faced off against each other.
[00:07:32] One of the strategies was called Friedman.
[00:07:34] It starts off by cooperating,
[00:07:36] but if its opponent defects just once,
[00:07:39] it will keep defecting for
the remainder of the game.
[00:07:42] Another strategy was called Joss.
[00:07:45] It also starts by cooperating,
[00:07:47] but then it just copies
[00:07:48] what the other player
did on the last move.
[00:07:50] Then around 10% of the time,
Joss gets sneaky and defects.
[00:07:55] There was also a rather elaborate strategy
[00:07:57] called Graaskamp.
[00:07:58] This strategy works the same as Joss,
[00:08:00] but instead of defecting
probabilistically,
[00:08:02] Graaskamp defects in the 50th round
[00:08:05] to try and probe the
strategy of its opponent
[00:08:07] and see if it can take
advantage of any weaknesses.
[00:08:10] The most elaborate
strategy was Name Withheld
[00:08:13] with 77 lines of code.
[00:08:17] After all the games were played,
the results were tallied up
[00:08:19] and the leaderboard established.
[00:08:21] - The crazy thing was
[00:08:23] that the simplest
program ended up winning,
[00:08:25] a program that came to
be called Tit for Tat.
[00:08:28] - [Derek] Tit for Tat
starts by cooperating
[00:08:31] and then it copies exactly
[00:08:32] what its opponent did in the last move.
[00:08:35] So it would follow
cooperation with cooperation
[00:08:37] and defection with defection,
[00:08:40] but only once if it's opponent
goes back to cooperating.
[00:08:43] So does Tit for Tat.
[00:08:45] When Tit for Tat played against Friedman,
[00:08:47] both started by cooperating
and they kept cooperating
[00:08:50] both ending up with perfect
scores for complete cooperation.
[00:08:54] When Tit for Tat played against Joss,
[00:08:56] they too started by cooperating
[00:08:58] but then on the sixth move, Joss defected.
[00:09:01] This sparked a series of
back and forth defections,
[00:09:03] a sort of echo effect.
[00:09:05] - Okay, so now you've got
this alternating thing
[00:09:08] which will remind you
of some of the politics
[00:09:10] of the world today where we
have to do something to you
[00:09:12] because of what you did to us.
[00:09:14] And then when this weird program throws
[00:09:16] in a second unprovoked
defection, now it's really bad
[00:09:20] because now both programs are
gonna defect on each other
[00:09:23] for the rest of the game.
[00:09:25] And that's also like some of the things
[00:09:27] that we're seeing in politics today
[00:09:29] and in international relations.
[00:09:31] - As a result of these
mutual retaliations,
[00:09:33] both Tit for Tat and Joss did poorly.
[00:09:36] But because Tit for Tat
managed to cooperate
[00:09:39] with enough other strategies,
[00:09:41] it still won the tournament.
[00:09:43] - As we are being joined...
[00:09:44] - Hey my God, there's Professor Axelrod.
[00:09:46] - Hey, there's Steven Strogatz.
[00:09:50] - Whoa, what a treat this is.
[00:09:52] - And I imagine initially it'd be sort
[00:09:55] of like computer chess
[00:09:55] where you need a pretty
complicated program
[00:09:57] to play a sophisticated game.
[00:09:59] But in fact it was not like that at all.
[00:10:01] It was the simplest
strategy that did the best.
[00:10:04] So I analyzed how that happened.
[00:10:08] - Axelrod found that all the
best performing strategies,
[00:10:10] including Tit for Tat,
shared four qualities.
[00:10:15] First, they were all nice,
[00:10:17] which just means they are
not the first to defect.
[00:10:20] So Tit for Tat is a nice strategy,
[00:10:22] it can defect but only in retaliation.
[00:10:25] The opposite of nice is nasty.
[00:10:28] That's a strategy that defects first.
[00:10:30] So Joss is nasty.
[00:10:32] Outta the 15 strategies in the tournament,
[00:10:34] eight were nice and seven nasty.
[00:10:37] The top eight strategies were all nice
[00:10:40] and even the worst
performing nice strategy
[00:10:42] still far outscored the
best performing nasty one.
[00:10:46] The second important
quality was being forgiving.
[00:10:50] A forgiving strategy is
one that can retaliate
[00:10:52] but it doesn't hold a grudge.
[00:10:54] So Tit for Tat is a forgiving strategy.
[00:10:57] It retaliates when its opponent defects
[00:10:59] but it doesn't let affections
[00:11:01] from before the last round
influence its current decisions.
[00:11:05] Friedman on the other hand,
is maximally unforgiving
[00:11:08] - After the first defection
just from the opponent
[00:11:12] would defect for the rest of the game.
[00:11:14] Okay, that's it.
[00:11:15] No mercy and that might feel good to do
[00:11:19] but it doesn't end up working
out well in the long run.
[00:11:23] - This conclusion that it pays to be nice
[00:11:25] and forgiving came as
a shock to the experts.
[00:11:28] Many had tried to be tricky
[00:11:30] and create subtle nasty
strategies to beat their opponent
[00:11:32] and eke out an advantage,
[00:11:35] but they all failed.
[00:11:36] Instead, in this tournament,
[00:11:38] nice guys finished first.
[00:11:40] Now Tit for Tat is quite forgiving
[00:11:41] but it's possible to
be even more forgiving.
[00:11:44] Axelrod's sample strategy only defects
[00:11:47] after its opponent
defected twice in a row.
[00:11:49] It was Tit for Two Tats.
[00:11:51] Now that might sound overly generous,
[00:11:54] but when Axelrod ran the numbers,
[00:11:56] he found that if anyone had
submitted the Sample strategy,
[00:11:59] they would've won the tournament.
[00:12:05] - I mean it's so clever,
[00:12:06] there's so many layers to this story.
[00:12:09] After Axelrod published his analysis
[00:12:11] of what happened or circulated it
[00:12:13] among these game theorists, he said,
[00:12:15] now that we all know what
worked well, let's try again.
[00:12:20] - So he announced a second tournament
[00:12:22] where everything would be the same except
[00:12:24] for one change the number
of rounds per game.
[00:12:28] See in the first tournament,
[00:12:29] each game lasted precisely 200 rounds.
[00:12:32] And that is important because if you know
[00:12:34] when the last round is,
[00:12:36] then there's no reason to
cooperate in that round.
[00:12:39] So you're better off defecting.
[00:12:41] Of course your opponent
should reason the same
[00:12:43] and so they should also
defect in the last round.
[00:12:46] But if you both anticipate
defection in the last round,
[00:12:49] then there's no reason for
you to cooperate in the second
[00:12:51] to last round or the round
before that, or before that
[00:12:54] and so on all the way
to the very first round.
[00:12:57] - And so in Axelrod's tournament,
[00:13:00] it was a very important thing
[00:13:01] that the players didn't know exactly
[00:13:03] how long they were gonna be playing.
[00:13:04] They knew on average
it would be 200 rounds,
[00:13:07] but there was a random number generator
[00:13:10] that prevented them from
knowing with certainty.
[00:13:13] - Yeah, if you're not sure when it ends,
[00:13:15] then you have to kind of keep cooperating
[00:13:17] 'cause it might keep going
and you need might need them
[00:13:18] on your side.
- That's right.
[00:13:21] - For this second tournament,
Axelrod received 62 entries
[00:13:25] and again, added random.
[00:13:27] The contestants had gotten the results
[00:13:29] and analysis from the first tournament
[00:13:30] and could use this information
to their advantage.
[00:13:33] This created two camps.
[00:13:35] Some thought that clearly being nice
[00:13:38] and forgiving were winning qualities.
[00:13:39] So they submitted nice
and forgiving strategies.
[00:13:42] One even submitted Tit for Two Tats.
[00:13:45] The second camp anticipated
that others would be nice
[00:13:48] and extra forgiving
[00:13:50] and therefore they
submitted nasty strategies
[00:13:52] to try to take advantage of
those that were extra forgiving.
[00:13:56] One such strategy was called Tester.
[00:13:58] It would defect on the first move
[00:14:00] to see how its opponent reacted.
[00:14:02] If it retaliated, tester would apologize
[00:14:04] and play Tit for Tat for
the remainder of the game.
[00:14:07] If it didn't retaliate,
[00:14:08] tester would defect every
other move after that.
[00:14:13] But again, being nasty didn't pay.
[00:14:16] - And once again, Tit for
Tat was the most effective.
[00:14:21] - Nice strategies again did much better.
[00:14:24] In the top 15, only one was not nice.
[00:14:27] Similarly, in the bottom
15, only one was not nasty.
[00:14:32] After the second tournament,
[00:14:34] Axelrod identified the other qualities
[00:14:36] that distinguished the
better performing strategies.
[00:14:38] The third is being retaliatory,
[00:14:41] which means if your opponent defects,
[00:14:43] strike back immediately,
don't be a pushover.
[00:14:47] Always cooperate is a total pushover.
[00:14:50] And so it's very easy
to take advantage of.
[00:14:52] Tit for Tat, on the other hand,
[00:14:54] is very hard to take advantage of.
[00:14:57] The last quality that Axelrod
identified is being clear.
[00:15:01] - Programs that were too opaque,
[00:15:03] that were too similar to a random program,
[00:15:06] you couldn't figure them out
[00:15:07] because they were so complicated,
[00:15:09] tt was very hard to establish
any pattern of trust
[00:15:12] with a program like that
[00:15:13] because you couldn't figure
out what it was doing.
[00:15:16] Not you.
[00:15:17] I mean the other programs it was playing
[00:15:18] couldn't figure them out and
so they would end up more
[00:15:21] or less defaulting to thinking
every turn is like the
[00:15:24] last time I'm gonna see you.
[00:15:25] So I might as well defect.
[00:15:27] What to me is mind blowing about this
[00:15:31] is that these four principles
being nice, forgiving,
[00:15:36] provokable and clear is
a lot like the morality
[00:15:40] that has evolved around the
world that is often summarized
[00:15:44] as an eye for an eye.
[00:15:46] It's not Christianity, by the way.
[00:15:48] It's not to not turn the
other cheek philosophy,
[00:15:51] it's some older philosophy.
[00:15:57] - What's interesting is that
[00:15:58] while Tit for Two Tats would've
won the first tournament,
[00:16:00] it only came 24th in
the second tournament.
[00:16:03] This highlights an important fact:
[00:16:06] in the repeated prisoner's dilemma,
[00:16:07] there is no single best strategy.
[00:16:10] The strategy that performs
best always depends
[00:16:13] on the other strategies
it's interacting with.
[00:16:15] For example, if you put Tit
for Tat in an environment
[00:16:18] with only the ultimate
bullies of always defect,
[00:16:21] then Tit for Tat comes in last.
[00:16:24] - I wanted to see whether, for example,
[00:16:27] the Tit for Tat did well
[00:16:28] because it did well
with really stupid rules
[00:16:31] that didn't do well with it all themselves
[00:16:33] that basically it took
advantage of people.
[00:16:35] - So he ran a simulation
[00:16:36] where successful strategies
in one generation
[00:16:38] would see their numbers
grow and unsuccessful ones
[00:16:41] would see their numbers drop.
[00:16:43] In this simulation,
[00:16:44] the worst performing
strategies quickly shrink
[00:16:47] and go extinct, while the
top performing strategies
[00:16:49] become more common.
[00:16:51] Harrington, the only nasty
strategy in the top 15,
[00:16:54] first grew quickly,
[00:16:56] but then as the strategies it
was preying on went extinct,
[00:16:59] Harrington's numbers also quickly dropped.
[00:17:03] This shows a main benefit
of this simulation
[00:17:06] because it tests how well a strategy does
[00:17:09] with other successful strategies.
[00:17:11] After a thousand generations,
[00:17:13] the proportions are mostly stable
[00:17:15] and only nice strategies survive.
[00:17:18] Again, Tit for Tat comes out on top,
[00:17:21] representing 14.5% of
the total population.
[00:17:25] Now this process may sound
similar to evolution,
[00:17:28] but there is a subtle difference,
[00:17:29] which is that in this case
there are no mutations.
[00:17:32] So it's actually an ecological simulation.
[00:17:35] But what if the world you
started in was different?
[00:17:39] - Imagine a world that is a
really nasty place to live,
[00:17:42] more or less populated with
players that always defect,
[00:17:45] except there's a little
cluster of tit-for-tat players
[00:17:48] that live in some kind of nucleus
[00:17:50] and they get to play with each other a lot
[00:17:51] because they're
geographically sequestered.
[00:17:54] They will start building
up a lot of points,
[00:17:57] and also because that
translates into offspring,
[00:17:59] they'll start to take over the population.
[00:18:02] So in fact, Axelrod showed
that a little island
[00:18:05] of cooperation can emerge and spread
[00:18:09] and eventually will take over the world,
[00:18:11] which is fantastic.
[00:18:14] How can cooperation emerge in a population
[00:18:19] of players who are self-interested?
[00:18:21] Who are not trying to be good
because they're good-hearted.
[00:18:25] You don't have to be altruistic.
[00:18:28] You could be looking out
for number one for yourself
[00:18:30] and your own interests.
[00:18:31] And yet cooperation can still emerge.
[00:18:34] (bright music)
[00:18:38] - Some argue that this
could explain how we went
[00:18:39] from a world full of
completely selfish organisms
[00:18:45] where every organism only
cared about themselves
[00:18:47] to one where cooperation
emerged and flourished.
[00:18:51] From impalas grooming each other
[00:18:53] to fish cleaning sharks.
[00:18:56] Many life forms experience
conflicts similar
[00:18:59] to the prisoner's dilemma,
[00:19:01] but because they don't interact just once,
[00:19:04] both can be better off by cooperating.
[00:19:08] And this doesn't require trust
or conscious thought either
[00:19:11] because the strategy
could be encoded in DNA,
[00:19:15] as long as it performs better
than the other strategies,
[00:19:18] it can take over a population.
[00:19:27] Axelrod's insights were applied
[00:19:28] to areas like evolutionary biology
[00:19:30] and international conflicts,
[00:19:32] but there was one aspect
[00:19:34] that his original
tournaments didn't cover.
[00:19:36] What happens if there is a little bit
[00:19:38] of random error in the game?
[00:19:40] Some noise in the system.
[00:19:42] For example, one player
tries to cooperate,
[00:19:44] but it comes across as a defection.
[00:19:47] Little errors like this happen
[00:19:48] in the real world all the time.
[00:19:50] Like in 1983,
[00:19:52] the Soviet satellite-based
early warning system
[00:19:54] detected the launch of an
intercontinental ballistic missile
[00:19:56] from the US but the US
hadn't launched anything.
[00:20:00] The Soviet system had confused
sunlight reflecting off
[00:20:03] high altitude clouds
with a ballistic missile.
[00:20:07] Thankfully, Stanislav Petrov,
[00:20:09] the Soviet officer on
duty, dismissed the alarm.
[00:20:12] But this example shows the
potential costs of a signal error
[00:20:15] and the importance of
studying the effects of noise
[00:20:18] on those strategies.
[00:20:20] - The word game sounds
like it's a children's game
[00:20:22] or, you know, there's some something,
[00:20:24] a misnomer maybe in calling it game theory
[00:20:27] because this is,
[00:20:28] these are life and
death matters obviously.
[00:20:31] And as you mentioned that
this came up in the Cold War.
[00:20:33] I mean it could actually be life and death
[00:20:36] of the whole planet,
[00:20:37] the whole we could annihilate
human civilization.
[00:20:39] So these are not games in
any kind of trivial sense,
[00:20:43] it's just the term that is used
[00:20:44] by mathematicians and theorists.
[00:20:47] - When Tit for Tat plays against itself
[00:20:49] in a noisy environment,
[00:20:50] both start off by cooperating,
[00:20:53] but if a single cooperation
is perceived as a defection,
[00:20:56] then the other Tit for Tat retaliates
[00:20:58] and it sets off a chain of
alternating retaliations.
[00:21:02] And if another cooperation
is perceived as a defection,
[00:21:05] then the rest of the game is
constant mutual defection.
[00:21:08] Therefore, in the long run,
both would only get a third
[00:21:11] of the points they would get
in a perfect environment.
[00:21:14] Tit for Tat goes from performing very well
[00:21:16] to performing poorly.
[00:21:19] So how do you solve this?
[00:21:21] Well, you need a reliable way
[00:21:23] to break out of these echo effects.
[00:21:25] And one way to do this is
by playing Tit for Tat,
[00:21:28] but with around 10% more forgiveness.
[00:21:31] So instead of retaliating
after every defection,
[00:21:34] you only retaliate around
nine out of every 10 times.
[00:21:38] This helps you break out of those echoes
[00:21:40] while still being retaliatory enough
[00:21:41] to not be taken advantage of.
[00:21:44] - And so we also ran the tournament
[00:21:46] with noise and generosity
and that did quite well.
[00:21:57] My favorite example is Tit
for Tat does really well,
[00:22:01] but it could never do better
[00:22:02] than the player it's playing with.
[00:22:05] - I mean, think about it, by design,
[00:22:07] all they can do is lose or draw.
[00:22:10] And yet when the results of all
interactions are tallied up,
[00:22:13] they come out ahead of
all other strategies.
[00:22:17] Similarly, always defect
can never lose a game.
[00:22:20] It can only draw or win,
[00:22:23] but overall, it performs extremely poorly.
[00:22:27] This highlights a common misconception
[00:22:29] because for many people when
they think about winning,
[00:22:31] they think they need to
beat the other person.
[00:22:34] In games like chess or poker, this is true
[00:22:37] since one person's gain
[00:22:38] is necessarily another person's loss,
[00:22:41] so these games are zero sum.
[00:22:43] But most of life is not zero sum.
[00:22:46] To win, you don't need to get your reward
[00:22:48] from the other player.
[00:22:49] Instead, you can get it from the banker.
[00:22:51] Only in real life, the
banker is the world.
[00:22:54] It is literally everything around you.
[00:22:57] It is just up to us to find
those win-win situations,
[00:23:00] and then work together
to unlock those rewards.
[00:23:04] Cooperation pays even among rivals.
[00:23:08] From 1950 to 1986,
[00:23:10] the US and Soviet Union
had trouble cooperating
[00:23:12] and both kept developing nukes.
[00:23:15] But then from the late '80s onwards,
[00:23:17] they started reducing
their nuclear stockpiles.
[00:23:21] They too had learned
how to resolve conflict.
[00:23:24] Rather than making an agreement
[00:23:25] to abolish all nuclear arms at once
[00:23:27] and essentially turning it into
a single prisoner's dilemma,
[00:23:31] they would disarm slowly, a
small number of nukes each year
[00:23:35] and then they'd check each other
[00:23:36] to see that they had both cooperated
[00:23:39] and then repeat the year after,
[00:23:41] and the year after that.
[00:23:43] All along, checking to
ensure mutual cooperation.
[00:23:47] In the more than 40 years
since Axelrod's tournaments,
[00:23:50] researchers have continued
[00:23:51] to study which strategies perform best
[00:23:53] in a variety of environments.
[00:23:55] In doing so, they varied
everything from payoff structures
[00:23:58] to strategies to errors and more.
[00:24:01] Some even allowed the strategies to mutate
[00:24:05] while Tit for Tat or generous Tit for Tat
[00:24:07] doesn't always come out on top,
[00:24:09] Axelrod's main takeaways still
hold: be nice, forgiving,
[00:24:13] but don't be a pushover.
[00:24:16] - Can I ask you, why did Anatol Rapoport
[00:24:19] submit Tit for Tat?
[00:24:23] - Well, the reason was
because I asked him to.
[00:24:26] (both laugh)
[00:24:28] And he wrote saying, yeah,
that I'm willing to do that,
[00:24:32] but I just wanna be
clear that I'm not sure
[00:24:35] that this is really such a good idea.
[00:24:36] I don't, he was a peace researcher
[00:24:39] and I think his own inclinations were
[00:24:42] to be much more forgiving and
maybe not be so provokable.
[00:24:49] - What I find fascinating is
that one of the main things
[00:24:52] that sets life apart
from non-living things
[00:24:54] that life gets to make decisions.
[00:24:56] We get to make choices.
[00:24:57] Choices that don't only change our future,
[00:25:00] but also the future of
those we interact with.
[00:25:02] You see, in the short term,
it is often the environment
[00:25:05] that shapes the player that
determines who does well.
[00:25:08] But in the long run, it is the players
[00:25:11] that shape the environment.
[00:25:14] So let's play a game, the game of life,
[00:25:17] and make your choices wisely
[00:25:19] because their impact may
reach further than you think.
[00:25:27] Using the right strategy matters,
[00:25:30] but figuring out the
best strategy isn't easy.
[00:25:33] It requires critical thinking
[00:25:34] and innovative solutions
like Axelrod's tournaments.
[00:25:37] If you are looking for an easy way
[00:25:39] to build your problem solving skills,
[00:25:40] then check out this
video's sponsor, Brilliant.
[00:25:43] Brilliant will help make
you a better thinker
[00:25:45] in everything from math and data science
[00:25:48] to programming, technology,
[00:25:50] you name it.
[00:25:51] You can start right now for free,
[00:25:53] straight from your device,
[00:25:54] the device you're watching this on.
[00:25:56] All you need to do is
set your learning goal
[00:25:58] and Brilliant will design
the perfect path for you,
[00:26:01] equipping you with all the
tools you need to reach it.
[00:26:04] Did you like today's
insights from game theory
[00:26:06] then Brilliant's brand new course,
[00:26:08] Intro to Probability is
the perfect next step.
[00:26:11] Introduction to
Probability is your gateway
[00:26:13] to mastering the tools of
chance, risk, and prediction.
[00:26:16] You'll learn how to construct
[00:26:17] and analyze models of
real world situations
[00:26:21] from electrons and business
decisions all the way
[00:26:23] to the 2023 Women's World Cup.
[00:26:26] Whether you are a budding statistician
[00:26:28] or you just wanna learn
about randomness and chance,
[00:26:31] this course will equip you
with the skills you need
[00:26:33] to make decisions in uncertain situations.
[00:26:36] You'll even take a page
out of Axelrod's Playbook
[00:26:38] and learn how to build
computer simulations
[00:26:41] to put your strategies to the test.
[00:26:43] Beyond probability, Brilliant
has a massive library
[00:26:45] of content covering everything
from math to data science
[00:26:48] and programming to technology.
[00:26:51] What I love about Brilliant is
that each lesson is hands-on,
[00:26:54] so you'll build real intuition
[00:26:56] and the best part is that you can learn
[00:26:57] with brilliant on the go.
[00:26:59] To try everything Brilliant
has to offer for free
[00:27:02] for a full 30 days visit
brilliant.org/veritasium
[00:27:06] and the first 200 of you to sign up
[00:27:08] will get 20% off Brilliant's
annual premium subscription.
[00:27:11] So I want to thank Brilliant
for sponsoring this video
[00:27:14] and I want to thank you for watching.
